{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwTte/guECIXN4zgpZ8pOD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knight19720208ui/AI/blob/main/qlearninn4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfGOSGH1lIzc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "20d8bf7f-5a4b-4315-e17b-5061e2477cdc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Matriz R\n",
        "R = np.matrix([ [-1,-1,-1,-1,0,-1],\n",
        "\t\t[-1,-1,-1,0,-1,100],\n",
        "\t\t[-1,-1,-1,0,-1,-1],\n",
        "\t\t[-1,0,0,-1,0,-1],\n",
        "\t\t[-1,0,0,-1,-1,100],\n",
        "\t\t[-1,0,-1,-1,0,100] ])\n",
        "\n",
        "# Matriz Q\n",
        "Q = np.matrix(np.zeros([6,6]))\n",
        "\n",
        "# Gamma (parametro de aprendizaje).\n",
        "gamma = 0.8\n",
        "\n",
        "# Estado inicial. (Usualmente es aleatorio)\n",
        "estadoInicial = 1\n",
        "\n",
        "# Retorna todas las posibles acciones del estado actual (pasado como parametro)\n",
        "def accionesDisponibles(estado):\n",
        "    filaEstadosActuales = R[estado,]\n",
        "    accionesDisponibles = np.where(filaEstadosActuales >= 0)[1]\n",
        "    return accionesDisponibles\n",
        "\n",
        "# Obtiene las acciones disponibles del estado actual\n",
        "accionesDisponiblesActualmente = accionesDisponibles(estadoInicial) \n",
        "\n",
        "# Retorna de forma aleatoria todas las posibles acciones para el estado actual\n",
        "def ejemploAccionesSiguientes(accionesDisponibles_range):\n",
        "    accionesSiguientes = int(np.random.choice(accionesDisponiblesActualmente,1))\n",
        "    return accionesSiguientes\n",
        "\n",
        "# Ejemplo de acciones que se pueden generar al momento\n",
        "accion = ejemploAccionesSiguientes(accionesDisponiblesActualmente)\n",
        "\n",
        "# Actualiza la matriz Q acorde a la ruta dada por el algoritmo Q \n",
        "def actualizar(estadoActual, accion, gamma):\n",
        "    \n",
        "    indiceMaximo = np.where(Q[accion,] == np.max(Q[accion,]))[1]\n",
        "\n",
        "    if indiceMaximo.shape[0] > 1:\n",
        "        indiceMaximo = int(np.random.choice(indiceMaximo, size = 1))\n",
        "    else:\n",
        "        indiceMaximo = int(indiceMaximo)\n",
        "    valorMaximo = Q[accion, indiceMaximo]\n",
        "    \n",
        "    # Q learning formula\n",
        "    Q[estadoActual, accion] = R[estadoActual, accion] + gamma * valorMaximo\n",
        "\n",
        "# actualizar Q matrix\n",
        "actualizar(estadoInicial,accion,gamma)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Entrenar\n",
        "\n",
        "# Entrenar cerca de 10 000 iteracciones. (Re-iterate el proceso de arriba).\n",
        "for i in range(10000):\n",
        "    estadoActual = np.random.randint(0, int(Q.shape[0]))\n",
        "    accionesDisponiblesActualmente = accionesDisponibles(estadoActual)\n",
        "    accion = ejemploAccionesSiguientes(accionesDisponiblesActualmente)\n",
        "    actualizar(estadoActual,accion,gamma)\n",
        "    \n",
        "# Normalize la matriz Q \"entrenada\"\n",
        "print(\"Matriz Q entrenada:\")\n",
        "print(Q/np.max(Q)*100)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Testing\n",
        "\n",
        "# Objetivo estado = 5\n",
        "# Mejor ruta iniciando en  2 ->:  2, 3, 1, 5\n",
        "\n",
        "estadoActual = 2\n",
        "steps = [estadoActual]\n",
        "\n",
        "while estadoActual != 5:\n",
        "\n",
        "    next_step_index = np.where(Q[estadoActual,] == np.max(Q[estadoActual,]))[1]\n",
        "    \n",
        "    if next_step_index.shape[0] > 1:\n",
        "        next_step_index = int(np.random.choice(next_step_index, size = 1))\n",
        "    else:\n",
        "        next_step_index = int(next_step_index)\n",
        "    \n",
        "    steps.append(next_step_index)\n",
        "    estadoActual = next_step_index\n",
        "\n",
        "# Print las secuencia/ruta tomada\n",
        "print(\"Ruta elegida:\")\n",
        "print(steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matriz Q entrenada:\n",
            "[[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.   80.    0. ]\n",
            " [  0.   80.   51.2   0.    0.  100. ]\n",
            " [  0.   80.    0.    0.   80.  100. ]]\n",
            "Ruta elegida:\n",
            "[2, 3, 4, 5]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}